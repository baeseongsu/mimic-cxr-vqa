import io
import os
import sys
import torch
import random
import pandas as pd
import pyarrow as pa

from PIL import Image
from pathlib import Path

import torchvision.transforms as transforms
from torchvision.transforms import Compose

sys.path.append(Path(__file__).parents[2])
from m3ae.transforms import keys_to_transforms
from prepro.make_arrow import make_arrow_mmehr


class BaseDataset(torch.utils.data.Dataset):
    def __init__(
            self,
            data_dir: str,
            transform_keys: list,
            image_size: int,
            names: list,
            text_column_name: str = "",
            max_text_len: int = 40,
            draw_false_image: int = 0,
            draw_false_text: int = 0,
            image_only: bool = False,
            label_column_name: str = "",
    ):
        super().__init__()
        assert len(transform_keys) >= 1
        # Hyper-Parameters
        self.text_column_name = text_column_name
        self.names = names
        self.max_text_len = max_text_len
        self.draw_false_image = draw_false_image
        self.draw_false_text = draw_false_text
        self.image_only = image_only
        self.data_dir = data_dir
        self.label_column_name = label_column_name
        
        # Image Transformations
        first_dataset_name = '_'.join(self.names[0].split('_')[:-1])
        if first_dataset_name in ['vqa_mmehr', 'mimic_cxr'] and len(self.names)==1:
            transform_keys = ['mmehr']
            self.transforms = keys_to_transforms(transform_keys, size=image_size)
            self.clip_transform = False
        else:
            if "train" not in names[0]:
                transform_keys = [transform_key.replace("_randaug", "") for transform_key in transform_keys]
                transform_keys = [transform_key.replace("_resizedcrop", "") for transform_key in transform_keys]
            self.transforms = keys_to_transforms(transform_keys, size=image_size)
            self.clip_transform = False
            for transform_key in transform_keys:
                if 'clip' in transform_key:
                    self.clip_transform = True
                    break

        # Read Texts
        if len(names) != 0:
            tables = [
                pa.ipc.RecordBatchFileReader(pa.memory_map(f"{data_dir}/{name}.arrow", "r")).read_all()
                for name in names
                if os.path.isfile(f"{data_dir}/{name}.arrow")
            ]

            if len(tables) == 0:
                tables = self.csv2arrow(data_dir, names)
                print(f"Load {len(tables)} csv file(s) instead of arrow file(s)...")

            self.table_names = list()
            for i, name in enumerate(names):
                self.table_names += [name] * len(tables[i])
            self.table = pa.concat_tables(tables, promote=True)
            if text_column_name != "":
                self.text_column_name = text_column_name
                self.all_texts = self.table[text_column_name].to_pandas().tolist()
                assert type(self.all_texts[0][0]) == str
            else:
                self.all_texts = list()
        else:
            self.all_texts = list()

        # Record Index Mappings
        self.index_mapper = dict()
        if text_column_name != "" and not self.image_only:
            j = 0
            for i, texts in enumerate(self.all_texts):
                for _j in range(len(texts)):
                    self.index_mapper[j] = (i, _j)
                    j += 1
        else:
            for i in range(len(self.table)):
                self.index_mapper[i] = (i, None)

    @property
    def corpus(self):
        return [text for texts in self.all_texts for text in texts]

    def __len__(self):
        return len(self.index_mapper)

    def get_raw_image(self, index, image_key="image"):
        index, caption_index = self.index_mapper[index]
        image_bytes = io.BytesIO(self.table[image_key][index].as_py())
        image_bytes.seek(0)
        if self.clip_transform:
            return Image.open(image_bytes).convert("RGBA")
        else:
            return Image.open(image_bytes).convert("RGB")

    def get_image(self, index, image_key="image"):
        image = self.get_raw_image(index, image_key=image_key)
        image_tensor = [tr(image) for tr in self.transforms]
        return {
            "image": image_tensor,
            "img_index": self.index_mapper[index][0],
            "cap_index": self.index_mapper[index][1],
            "raw_index": index,
        }

    def get_false_image(self, rep, image_key="image", selected_index=None):
        random_index = random.randint(0, len(self.index_mapper) - 1)
        image = self.get_raw_image(random_index, image_key=image_key)
        image_tensor = [tr(image) for tr in self.transforms]
        return {f"false_image_{rep}": image_tensor}

    def get_text(self, raw_index):
        index, caption_index = self.index_mapper[raw_index]
        text = self.all_texts[index][caption_index]
        encoding = self.tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=self.max_text_len,
            return_special_tokens_mask=True,
            return_offsets_mapping=True,
        )
        return {
            "text": (text, encoding),
            "img_index": index,
            "cap_index": caption_index,
            "raw_index": raw_index,
        }

    def get_false_text(self, rep, selected_index=None):
        random_index = random.randint(0, len(self.index_mapper) - 1)
        index, caption_index = self.index_mapper[random_index]
        text = self.all_texts[index][caption_index]
        encoding = self.tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=self.max_text_len,
            return_special_tokens_mask=True,
            return_offsets_mapping=True,
        )
        return {f"false_text_{rep}": (text, encoding)}

    def get_suite(self, index):
        result = None
        while result is None:
            try:
                ret = dict()
                ret.update(self.get_image(index))
                if not self.image_only:
                    txt = self.get_text(index)
                    ret.update({"replica": True if txt["cap_index"] > 0 else False})
                    ret.update(txt)
                for i in range(self.draw_false_image):
                    ret.update(self.get_false_image(i, selected_index=index))
                for i in range(self.draw_false_text):
                    ret.update(self.get_false_text(i, selected_index=index))
                result = True
            except Exception as e:
                print(f"Error while read file idx {index} in {self.names[0]} -> {e}")
                index = random.randint(0, len(self.index_mapper) - 1)
        return ret

    def collate(self, batch, mlm_collator):
        batch_size = len(batch)
        keys = set([key for b in batch for key in b.keys()])
        dict_batch = {k: [dic[k] if k in dic else None for dic in batch] for k in keys}

        img_keys = [k for k in list(dict_batch.keys()) if "image" in k]
        img_sizes = list()

        for img_key in img_keys:
            img = dict_batch[img_key]
            img_sizes += [ii.shape for i in img if i is not None for ii in i]

        for size in img_sizes:
            assert (len(size) == 3), f"Collate error, an image should be in shape of (3, H, W), instead of given {size}"

        if len(img_keys) != 0:
            max_height = max([i[1] for i in img_sizes])
            max_width = max([i[2] for i in img_sizes])

        for img_key in img_keys:
            img = dict_batch[img_key]
            view_size = len(img[0])
            new_images = [torch.zeros(batch_size, 3, max_height, max_width) for _ in range(view_size)]
            for bi in range(batch_size):
                orig_batch = img[bi]
                for vi in range(view_size):
                    if orig_batch is None:
                        new_images[vi][bi] = None
                    else:
                        orig = img[bi][vi]
                        new_images[vi][bi, :, : orig.shape[1], : orig.shape[2]] = orig
            dict_batch[img_key] = new_images

        txt_keys = [k for k in list(dict_batch.keys()) if "text" in k]
        if len(txt_keys) != 0:
            encodings = [[d[1] for d in dict_batch[txt_key]] for txt_key in txt_keys]
            flatten_encodings = [e for encoding in encodings for e in encoding]
            flatten_mlms = mlm_collator(flatten_encodings)

            for i, txt_key in enumerate(txt_keys):
                texts, encodings = ([d[0] for d in dict_batch[txt_key]], [d[1] for d in dict_batch[txt_key]])
                mlm_ids, mlm_labels = (
                    flatten_mlms["input_ids"][batch_size * (i): batch_size * (i + 1)],
                    flatten_mlms["labels"][batch_size * (i): batch_size * (i + 1)],
                )

                input_ids = torch.zeros_like(mlm_ids)
                attention_mask = torch.zeros_like(mlm_ids)
                for _i, encoding in enumerate(encodings):
                    _input_ids, _attention_mask = (
                        torch.tensor(encoding["input_ids"]),
                        torch.tensor(encoding["attention_mask"]),
                    )
                    input_ids[_i, : len(_input_ids)] = _input_ids
                    attention_mask[_i, : len(_attention_mask)] = _attention_mask

                dict_batch[txt_key] = texts
                dict_batch[f"{txt_key}_ids"] = input_ids
                dict_batch[f"{txt_key}_labels"] = torch.full_like(input_ids, -100)
                dict_batch[f"{txt_key}_ids_mlm"] = mlm_ids
                dict_batch[f"{txt_key}_labels_mlm"] = mlm_labels
                dict_batch[f"{txt_key}_masks"] = attention_mask

        return dict_batch

    def csv2arrow(self, data_dir, names):
        csvs = [
                pd.read_csv(f"{data_dir}/{name}.csv", low_memory=False)
                for name in names
                if os.path.isfile(f"{data_dir}/{name}.csv")
            ]

        IMAGE_ROOT = "../../../physionet.org/files/mimic-cxr-jpg/2.0.0/re512_3ch_contour_cropped"
        dataset_name = "vqa_mmehr"
        arrows = []
        questions = []

        for samples, name in zip(csvs, names):
            q2id = {k: i for i, k in enumerate(set(samples.question))}
            samples["qid"] = samples["question"].apply(lambda x: q2id[x])
            samples["answer"] = samples["answer"].apply(lambda x: [] if pd.isna(x) else str(x).lower().split("|"))

            for sample_idx, sample in samples.iterrows():
                img_path = os.path.join(IMAGE_ROOT, sample["image_path"])
                question = sample["question"]
                qid = sample["qid"]
                answer = sample["answer"]
                answer_type = "CLOSED"  # TODO: open/closed
                question_type = sample["category_type"]  # TODO: open/closed
                questions.append({
                    "img_path": img_path,
                    "qid": qid,
                    "question": question,
                    "answer": answer,
                    "answer_type": answer_type,
                    "question_type" : question_type,
                })
            
            assert dataset_name in name
            split = name.split(f"{dataset_name}_")[-1] #NOTE: hard coding

            arrows.append(
                make_arrow_mmehr(questions, split, dataset_name, "", save=False)
                )

        return arrows
